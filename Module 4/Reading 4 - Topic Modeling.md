# Seeing Patterns

In this module we are asked to make a connection between the reading given to us and our work in other history classes. As I am not a historian and the last history class I had was in grade 8, I will try to apply it to my computer science research.

## Topic Modeling

The first article I read for this topic mentioned that digital humanists rarely use k-means and k-nearest neightbour algorithms. These are algorithms that require prior knowledge of the number of clusters into which to cluster the data, which may be why they are rarely used. They are also very simple algorithms that use averages to adjust the cluster centroids (the middle point of a cluster) to a position that is actually in the middle of a cluster rather than its randomly initialised position. 

This article focuses rather on the use of an topic modeling algorithm called LDA (Latent Dirichlet Allocation). This is not an algorithm I can discuss in depth as I haven't actually used it before myself. The author uses the algorithm on a ship's dataset of geographical points and mapped the results onto a world map. The results surprised the author, the algorithm performed much better than expected and gave clear separations between the various topics.

Because topic modeling also requires prior knowledge of the number of topics (or clusters), the results are highly dependent on a good selection of the number of topics. One way researchers get around this is running the algorithm multiple times with different values given. This to me feels a bit like guess work. This is exactly the topic I tackled in my thesis, where I worked on an algorithm that doesn't require prior knowledge of the total number of clusters. There are various options out there regarding the automated determination of the total number of clusters. I personally investigated using multi-population differential evolution algorithms to do this. The number of clusters were correctly determined by my algorithm (yeii :D) but on certain larger data sets the Artificial Immune System I was comparing to performed better. Additionally, as my algorithm was a multi-population algorithm, it was quite slow the more clusters it created (as every new cluster resulted in a population).

I noticed in the article that the author speculates about the reasoning behind these results. In my case the results when diminishing the number of clusters is obvious as I know the inner workings of common clustering algorithms. This is not to say that my speculation is correct as I don't know the inner workings of that particular one. However, most clustering algorithms, including K-means and the ones I had to deal with in my thesis, use the approach of moving centroids around the search space of the dataset and assigning data to each centroid. I won't  get into more detail as each  algorithm does this differently. However, in summary: less centroids (or clusters) = more data being assigned to the same centroid = more data being processed as being "related" to each other.

Lastly, the author also mentions that digital humanists often just throw data at MALLET blindly without really adjusting the parameters the algorithms require and thinking more thoroughly about their problem. This is also a weakness Computational Intelligence algorithms have but are not explained to the general public. These algorithms need parameters to be adjusted in order for them to work for one's particular problem. They are not magical and don't know everything, contrary to popular belief. This is where the "there is no free lunch theorem" comes in, there isn't one algorithm that can solve all your problems. The algorithm and parameters you choose always depend on the problem you are trying to solve.

The second article (which I was meant to read first) explains topic modeling used for text analysis. It mentions the importance of understanding the data one is modeling and breaks down the process into 4 steps: pre-processing, indexing, modeling and analysis. The author explains some techniques for these steps, such as removing words such as "and", "the", etc during pre-processing of the data; and using term frequency during modeling.
